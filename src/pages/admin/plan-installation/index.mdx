---
title: Plan Installation
---

<PageDescription>

Prepare to install Developer Tools environments

</PageDescription>

## Overview

As explained in the [overview](/overview), the Developer Tools environment consists of the following components and developer tools:
- A development cluster
- A set of backend services
- A set of continuous delivery tools

This diagram illustrates the Development Tools environment:

![Provisioned environment](./images/catalyst-provisioned-environment.png)

The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools.

### Dev tools inside

The developer tools implement the CI/CD pipeline for continuous delivery. They are deployed inside the development cluster in the tools namespace, which has several advantages.

**The tools are part of the environment.** The pipeline has to be hosted somewhere. Rather than find some new place for it in the cloud, or worse yet outside the cloud, let's host the pipeline in the Developer Tools environment. The environment includes this development cluster--let's run the pipeline in there. Since Kubernetes organizes resources in namespaces, let's put these developer tools in their own namespace, we'll call it *tools*.

**The tools go where the environment goes.** Because the developer tools and the pipeline they implement are hosted in the cluster, the pipeline can be installed wherever the environment can be installed--which is any Kubernetes or Red Hat OpenShift cluster. This is especially important for hybrid- and multi-cloud architectures where cloud environments are hosted on different platforms--IBM Cloud, AWS, Azure, on-prem, etc. The environment cannot depend on the CI/CD tools provided by a single platform, it needs tools that can be hosted on any platform.

**Each team selects its own tools.** Because the developer tools are installed separately in each environment, the development team that owns the environment can customize the tools and pipeline specifically for their project. All of the development teams in the enterprise don't have to agree on a single set of tools and a single set of pipeline configurations. Each team can customize their toolset to meet the team's preferences.

**Each team manages its own tools.** Because the developer tools are hosted inside the environment, the enterprise's operation teams don't have to be responsible for managing the tools. Each development team can manage its own tools running in its own environment. This is especially significant when various development teams select a wide range of tools. Normally operations would be responsible for managing all of the tools that any team wants to use, and would scope their responsibilities by limiting the tools teams can select. Now each team can decide which tools it wants to use, as long as they can be hosted in Kubernetes and the team is willing to pay for them and manage them.

**No production application dependencies.** When an application developed in a Developer Tools environment is deployed to production, it does not require the continuous delivery pipeline in the production environment. The application is deployed using the artifacts produced by the pipeline--principally Helm charts of Docker images--but the pipeline is not required in the production environment. That is all the more reason to host the pipeline in the development environment, the only place it is needed.

### Ops tools outside

The backend services provide capabilities the applications depend on, which will be needed in production. They should be provided by the platform, so this part of the environment is platform-specific. The production instances of these services will need to be managed by the enterprise's operations teams. These services are an agreement, a point of coordination between the development team and the operations team. There are several aspects to this agreement.

**Monitoring and management**: Applications should be [built to manage](https://www.ibm.com/garage/method/practices/code/build-to-manage). This requires monitoring systems outside the application, applications designed for monitoring, and operations personnel who use the monitoring. The Developer Tools environment uses two of the primary monitoring services in the IBM Cloud service catalog: [IBM Cloud Monitoring with Sysdig](https://cloud.ibm.com/docs/services/Monitoring-with-Sysdig) and [IBM Log Analysis with LogDNA](https://cloud.ibm.com/docs/services/Log-Analysis-with-LogDNA).

**Data persistence**: Cloud applications are stateless, but user functionality has data--that data must be stored somewhere. It should be stored and managed outside of the application so that the data survives application upgrades and outages. Applications typically have three types of data, and the Developer Tools environment incorporates IBM Cloud services for each one: [Databases For PostgreSQL](https://cloud.ibm.com/docs/services/databases-for-postgresql) for legacy relational data, [IBM Cloudant](https://cloud.ibm.com/docs/services/Cloudant) for document data, and [Cloud Object Storage](https://cloud.ibm.com/docs/services/cloud-object-storage) for binary data.

**Security**: Applications need to not only provide user functionality, but to do so securely. One important aspect of security is authenticating the users of the application. The Developer Tools environment provides this capability using [IBM Cloud App ID](https://cloud.ibm.com/docs/services/appid).

### Teams for developers

Each development team gets its own Developer Tools environment. This gives each team its own isolated development compute space so that teams can work independently from each other without interfering with each other. As noted above, each team can customize its environment without interfering with other teams or burdening operations. When two teams want to independently work on separate applications, give them separate environments.

## Planning team topology

Having said that separate development teams can have separate Developer Tools environments, how many environments does a set of teams need? There are two approaches.

**Cluster per team**: As described above, each development team gets its own Developer Tools environment. This provides independent resources that can be managed and even billed separately.

**Shared cluster**: Multiple development teams can share a single Developer Tools environment, thereby sharing the resources, their cost and management duties. In the single environment, each development team defines and uses its own cluster namespace: *dev-team1*, *dev-team2*, etc. They then all use Argo CD--one of the Developer Tools--to deploy to shared *test* and *staging* namespaces. This approach is especially helpful for applications developed by separate teams that need to work together in production.

## Prepare IBM Cloud account

The account must provide a few resources that will be needed to install the Developer Tools environment: a public/private pair of VLANs, a resource group, and a pair of access groups for the admin and developers. These will be created by an account administrator, either the owner of the account or a user in the account with Administrator roles.


### Data center

First, decide which [IBM Cloud location](https://cloud.ibm.com/docs/overview?topic=overview-zero-downtime#ov_intro_reg "Locations for resource deployment") will host the Developer Tools environment. That will be specified with two settings:
- Region -- A geography such as *us-south* or *eu-gb*
- Zone -- A [data center](https://cloud.ibm.com/docs/overview?topic=overview-zero-downtime#data_center) in the region such as *dal10* or *lon02*


### Public and private VLANs

Second, create or provide a pair of public and private VLANs for the selected region and zone. These VLANs will implement the public and private networks in the Kubernetes or OpenShift cluster.

<InlineNotification>

Note: An account typically automatically has a pair of VLANs for each region and zone. If so, you can use those.

</InlineNotification>

- Public and private VLANs -- [List existing VLANs](https://cloud.ibm.com/classic/network/vlans)
    - [Getting started with VLANs](https://cloud.ibm.com/docs/infrastructure/vlans)
    - [Understanding network basics of classic clusters](https://cloud.ibm.com/docs/containers?topic=containers-plan_clusters#plan_basics)
    - [Overview of classic networking in IBM Cloud Kubernetes Service](https://cloud.ibm.com/docs/containers?topic=containers-subnets#basics)
- Use the [IGC CLI](/getting-started/cli)'s `igc vlan` command to verify it can find two VLANs to use


### Resource group

Third, create or provide a [resource group](https://cloud.ibm.com/docs/resources?topic=resources-rgs). This resource group will control access to the environment's cluster and service instances. This resource group should typically be named after the development team, its project, or the application it is implementing.

<InlineNotification kind="warning">

**Warning**: Because of a current limitation in the installation scripts, **the resource group's name should conform to [Kubernetes resource naming conventions](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names)**--the name should be all lowercase letters, digits, and the separators should be dashes.

</InlineNotification>

- [Create the resource group](https://cloud.ibm.com/account/resource-groups)

To create clusters in the resource group, the account will need API keys for the container service to create resources in the classic infrastructure. A separate API key is needed for each region and resource group. [Classic infrastructure roles](https://cloud.ibm.com/docs/containers?topic=containers-access_reference#infra) explains the classic infrastructure permissions the cluster service needs. This key grants these permissions to the cluster service, so that any user with permissions to create a cluster can do so without requiring any classic infrastructure permissions.

- Create an API key for the resource group and the data center's region
    - Perform these steps to create an API key: [Setting up the API key to enable access to the infrastructure portfolio](https://cloud.ibm.com/docs/containers?topic=containers-users#api_key)
    - The [list of existing API keys](https://cloud.ibm.com/iam/apikeys) shows the new key named `containers-kubernetes-key`


### Prepare to run scripts

Fourth, we'll use some scripts in the steps below to help create access groups. Here, we'll download the scripts and prepare to run them. (If you want to use the console to manually configure the access groups, you can skip this step.)

Clone the Git repository with the scripts. (This repo also has the scripts for installing the Developer Tools environment.)

- Clone the [ibm-garage-iteration-zero](https://github.com/ibm-garage-cloud/ibm-garage-iteration-zero) Git repository to your local filesystem
    ```bash
    git clone git@github.com:ibm-garage-cloud/ibm-garage-iteration-zero.git
    ```

- Switch to the cloned directory
    ```bash
    cd ibm-garage-iteration-zero
    ```

The scripts need you to log into IBM Cloud first. In the terminal you'll use to run the scripts, log in to IBM Cloud.

- Log in to the IBM Cloud CLI
    ```bash
    ibmcloud login -a cloud.ibm.com -r <region> -g <resource-group>
    ```


### Access group for administrators

Fifth, create an access group to grant the necessary permissions for installing a Developer Tools environment. Do this by running a script or using the console. Also, add the environment administrator(s) (who is the user who will run the scripts to create the environment) to this group.

- Use the admin policies script

    - Create a new [access group](https://cloud.ibm.com/docs/iam?topic=iam-account_setup), name it something like `<resource_group>-DEV` (all capital letters)
    - Run the script `./terraform/scripts/acp-admin`, which will add the necessary policies to the access group
    - Add the environment administrator(s) to the group

- Or instead of using the script, use the console to manually add the policies to the user or access group

    - Permission to create clusters
    - Permission to admin the IBM Cloud Container Registry
    - Permission to create service instances (shown below)
    ![Assign Access](/images/assign-access-all-services.png)

<InlineNotification kind="warning">

**Warning**: There's currently a bug in one of the ibmcloud commands that the script uses. The command creates the Viewer policy with an extra "All service in" clause and doesn't work.

</InlineNotification>

If the script was used, the resource Viewer policy looks like this:

```
All service in RESOURCE_GROUP_NAME resource group
resourceType string equals resource-group, resource string equals RESOURCE_GROUP_NAME
```

Use the console to manually add a second resource Viewer policy that doesn't contain the extra "All service in" clause. The resulting resource Viewer policy looks like this:
```
RESOURCE_GROUP_NAME resource group
resourceType string equals resource-group, resource string equals RESOURCE_GROUP_NAME
```


### Access group for developers

Sixth, create an access group to enable developers to access the Developer Tools environment. This can be done later, after the environment is created, either by running a script or using the console. Also, add the developers (the users who will use the environment) to this group.

- Use the dev policies script

    - Create a new [access group](https://cloud.ibm.com/docs/iam?topic=iam-account_setup), name it something like `<resource_group>-DEV` (all capital letters)
    - Run the script `./terraform/scripts/acp-dev`, which will add the necessary policies to the access group
    - Add the developers to the group

- Or instead of using the script, use the console to manually add the policies to the access group
    - Access to the resource group
    - Access to the cluster
    - Access to the image registry
    - Editor and Manager (but not Administrator) access to each of the services in the resource group

<InlineNotification kind="warning">

**Warning**: Like you did for the administrators' access group, if you use the script to populate the policies in this access group, use the console to manually add a second resource Viewer policy.

</InlineNotification>




## Cloud Preqs

## Setup steps

- Create Resource Group
- Create access group (capatilised )
- Create Cluster in the resource group name it `<team>-iks|ocp-cluster`
    - 3 Node and 8cpu and 33gb memory (typically for a sensibel team)
- Install Developer Tools into cluster following install existing guide
- run `terraform/scripts/Usage: acp <ACCESS_GROUP> <RESOURCE_GROUP> <PREFIX> <REGION>`
- apply RBAC Rules`terraform/scripts/rbac.sh <ACCESS-GROUP>`
- Test `igc dashboard`
- Complete setup of LogDNA
- Complete setup of SysDig
- Deploy a first app, test end to end
- Complete Artifactory Setup
- Complete ArgoCD Setup
- Rerun pipeline to check app helm chart is added to Artifactory
- Setup a GitOps repo to validate ArgoCD setup and configuration


